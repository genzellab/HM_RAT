# ğŸ® Stitch_Sync_Track â€” Hexmaze Rat Project

This folder integrates **multi-camera video synchronization**, **stitching**, and **automated rat tracking** for the **Hexmaze Rat** experiment.  
The goal is to produce a **fully synchronized behavioral dataset** â€” where every tracked rat position is precisely aligned to the neural recordings.

Importantly, **temporal synchronization** happens *before* stitching.  
Each camera video is aligned to the neural clock via LEDâ€“DIO regression, and these per-eye regressions are combined into a **single global timestamp series** that defines the true frame-by-frame timing of the final stitched video.

---

## ğŸ§  Conceptual Overview

The workflow combines behavioral videos and electrophysiology data through the following stages:

```
Multi-camera videos
â†’ LEDâ€“DIO synchronization via ICA regression (per eye)
â†’ Averaged global timestamps (stitched_framewise_ts.csv)
â†’ Visual stitching of videos
â†’ Tracker applies timestamps to behavioral detections
â†’ Neuralâ€“behavioral dataset
```

---

## ğŸ§¬ Full Hex-Maze Video Processing Pipeline

This pipeline transforms raw multi-camera recordings of the **Hex-Maze** experiment into fully timestamped, analyzed videos that show the ratâ€™s path across trials â€” all in true neural time.

It consists of three main stages:  
**1. Synchronization**, **2. Stitching**, and **3. Tracking**.

---

### ğŸ•¹ï¸ Step 1 â€” Synchronization (`Video_LED_Sync_using_ICA.py`)

Each eye camera records independently, with its own clock that may drift slightly relative to the neural recording system.  
This script aligns all video frames to the **neural clock** using the LEDâ€“DIO system and builds the global timestamp mapping used later.

**How it works**
1. For each eye video, extracts a small LED region and applies **Independent Component Analysis (ICA)** to isolate the LED signal.  
2. Reads corresponding LED control pulses from the **neural DIO (.dat)** files, which store the ground-truth timing of LED blinks.  
3. Builds a **linear regression model** mapping each cameraâ€™s internal frame time to the DIO timestamps, correcting for offset and drift.  
4. **Averages** the corrected per-eye timelines to produce a **global framewise timestamp series** shared across all cameras.  
5. Outputs a single CSV (`stitched_framewise_ts.csv`) that defines the corrected, neural-aligned time for every frame in the stitched video.

**Output**
- ğŸ—‚ï¸ `stitched_framewise_ts.csv` â€” table indexed by frame number with the column `"Corrected Time Stamp"`.  
  Each entry represents the **true neural-aligned timestamp** of that stitched frame, averaged across all eye regressions.

> âœ… This file is the core synchronization reference for all downstream steps.  
> âŒ It does *not* visually synchronize or merge the videos â€” it only defines time alignment.

---

### ğŸ¥ Step 2 â€” Join Views (`join_views.py`)

The maze is recorded by multiple fixed cameras (typically 12).  
This script visually merges them into a single panoramic view for manual inspection and automated tracking.

**How it works**
1. Finds all individual eye videos in the target folder.  
2. Aligns their **first frame (frame 0)** to start simultaneously.  
3. Ends the output at the shortest input video duration.  
4. Optionally crops or flips videos to maintain consistent maze orientation.  
5. Uses **FFmpeg overlay filters** to tile the videos in a 2Ã—6 grid and generate one stitched video.

**Output**
- ğŸ® `stitched.mp4` â€” a single panoramic video combining all eye views.

> âš ï¸ This stage performs **no temporal synchronization** â€” it simply places all camera videos side by side for visual convenience.  
> The precise neural timing is recovered later via the `stitched_framewise_ts.csv` file from Step 1.

---

### ğŸ€ Step 3 â€” Tracking (`TrackerYolov3-Colab.py`)

The tracker performs automatic rat detection and tracking on the **stitched video** using a trained **YOLOv3 ONNX** network.  
Each frame processed by the tracker is then assigned its **true neural timestamp** using the global timestamp mapping from synchronization.

**How it works**
1. Loads both `stitched.mp4` and `stitched_framewise_ts.csv`.  
2. Detects the ratâ€™s head, body, and the experimenter using YOLOv3 inference.  
3. Tracks the ratâ€™s movement across maze nodes (`node_list_new.csv`).  
4. For each processed frame, retrieves the corresponding `"Corrected Time Stamp"` from the CSV â€” the **global averaged timestamp** computed during synchronization.  
5. Logs detections and behavioral events using these neural-aligned timestamps.  
6. Annotates and saves the stitched video with trajectories, node IDs, and trial information.

**Outputs**
- ğŸ­ `<date>_Rat<ID>.mp4` â€” tracked video with path annotations.  
- ğŸ–œ `log_<date>_Rat<ID>.log` â€” detailed frame-by-frame detections, each labeled with the neural-synchronized timestamp.  
- ğŸ“ˆ *(optional)* `<date>_Rat<ID>.txt` â€” session summary (nodes, durations, velocities, etc.).

---

### ğŸ§© Putting It All Together

| Stage | Inputs | Outputs | Purpose |
|:------|:--------|:---------|:---------|
| **1. Synchronization** | Eye videos + DIO signals | `stitched_framewise_ts.csv` | Compute neural-aligned timestamps for each frame (averaged across eyes) |
| **2. Join Views** | Eye videos | `stitched.mp4` | Merge all cameras into a visual grid (no timing correction) |
| **3. Tracker** | `stitched.mp4` + `stitched_framewise_ts.csv` | Tracked video + logs | Detect and log rat behavior in true neural time |

---

### ğŸ§¤ Key Insight

The **synchronization step** defines the true neural timeline using averaged per-eye LEDâ€“DIO regressions.  
The **stitched video** is only a visual montage â€” it has no inherent timing correction.  
Finally, the **tracker** fuses both by applying the neural-aligned timestamps from `stitched_framewise_ts.csv` to every frame and event.

ğŸŸ¢ **In short:**  
- Synchronization = builds the global neural timeline.  
- Stitching = creates the visual grid.  
- Tracking = overlays rat detections and applies neural timestamps.

---

### ğŸ§¬ Final Outcome

At the end of the pipeline you obtain:
- A single **stitched video** showing all camera views.  
- A **tracked video** with the ratâ€™s trajectory and trial annotations.  
- **Logs and summaries** where every event is timestamped in neural time, enabling precise behavioralâ€“electrophysiological analysis.

---

## ğŸ“Š Output Files

| File | Description |
|------|--------------|
| `stitched.mp4` | Multi-camera panoramic video (visual only) |
| `stitched_framewise_ts.csv` | Global frame-to-neural timestamp table |
| `log_<date>_Rat<ID>.log` | Frame-by-frame detections with synchronized timestamps |
| `<date>_Rat<ID>.mp4` | Tracked video with annotated rat path |
| `<date>_Rat<ID>.txt` | Optional session summary (nodes, durations, velocities) |

---

## ğŸ“¦ Dependencies

From `requirements.txt`:

```
opencv-python
matplotlib
onnxruntime-gpu
scikit-learn
pandas
```

Install via:
```bash
pip install -r requirements.txt
```

---

## ğŸ‘¥ Credits

Developed under the supervision of **[Dr. AdriÃ¡n AlemÃ¡n Zapata](https://github.com/Aleman-Z)**  

**Contributors:**
- **Giulia Porro** â€” YOLOv3 tracker integration  
- **Param Rajpura** â€” GPU acceleration and synchronization implementation (ICAâ€“DIO regression)  
- **Olivier Peron** â€” electrophysiologyâ€“video synchronization validation  
- **Ã–zge Ã‡ekirge** â€” ICA-based synchronization analysis  
- **Daniela Morales** â€” Colab and FFMPEG optimization  
- **Genzel Lab Team** â€” experimental data and conceptual design  

---

## ğŸŸ’ License

Â© **Genzel Lab** â€” for research use only.  

This folder combines three modules â€” **Synchronization**, **Stitching**, and **Tracking** â€” into a single executable workflow.  
Usage and Installation Manual:  
[Installation and Usage Manual â€“ Dropbox Link](https://www.dropbox.com/scl/fi/wwph1cjct5o0m6cffelm3/Installation-and-Usage-Manual-for-HM-Stitch-Sync-Track.docx?rlkey=drsd7q3debt064n3khcz4tlji&st=qymj8ie4&dl=0)

---

ğŸ“Œ *Each frame in the stitched video inherits its timestamp from the averaged neural-aligned series (`stitched_framewise_ts.csv`), ensuring that all tracked rat positions are expressed in true experimental time.*

