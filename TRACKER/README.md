## HM TRACKER 
# ğŸ¯ Hexmaze Rat â€” TRACKER Module  

GPU-accelerated rat tracking system based on **YOLOv3** for multi-camera behavioral recordings in the Hexmaze.  
This module extracts rat trajectories from stitched videos and synchronizes them with electrophysiological data.

---

## ğŸ‘¥ Credits

Developed under the supervision of **[Dr. AdriÃ¡n AlemÃ¡n Zapata](https://github.com/Aleman-Z)**.  

**Contributors:**  
- **Giulia Porro** â€” implemented the original YOLOv3 tracker and Colab version  
- **Param Rajpura** â€” implemented GPU acceleration and optimized synchronization with electrophysiological data  
- **Daniela Morales** â€” supported GPU optimization and computational setup  
- **Genzel Lab Team** â€” integration and validation with multi-camera recordings  

---

## âš™ï¸ Overview

This module detects and tracks rats across videos recorded from the **Hexmaze setup (12 cameras)**.  
It uses a YOLOv3-based detection model to extract trajectories, running efficiently on both **local GPU machines** and **Google Colab**.

The tracking pipeline includes:
1. **Model initialization** with pre-trained YOLOv3 weights (.onnx or .rar)
2. **GPU-based inference** for rapid frame-by-frame detection
3. **Trajectory logging** into `.csv` and `.txt` outputs
4. **Synchronization** with electrophysiological timestamps for joint behavioralâ€“neural analysis

---

## ğŸ’» Installation & Dependencies

### 1. Environment Requirements
- **Python â‰¥ 3.8**  
- **CUDA â‰¥ 11.6**  
- **cuDNN â‰¥ 8.5.0.96**  
- **ONNX Runtime GPU (tested with v1.8.1)**  
- **TensorFlow GPU (optional for hardware validation)**  

### 2. Python Packages
```bash
pip install onnxruntime-gpu==1.8.1 tensorflow pyunpack opencv-python tqdm
```

---

## ğŸš€ Usage

### ğŸ”¹ Local GPU Execution

```bash
python TRACKER_GPU.py -i /path/to/video/stitched.mp4 -o /path/to/output/folder
```

**Arguments:**
- `-i` : path to input stitched video (e.g., from `video_stitching/`)  
- `-o` : output directory containing `/logs` and `/videos` subfolders  

Example:
```bash
python TRACKER_GPU.py -i ./stitched_videos/Rat5_20210614_presleep.mp4 -o ./tracker_output/
```

---

### ğŸ”¹ Running on Google Colab

You can also execute the tracking pipeline on **Google Colab** using the provided notebooks:
- **`Tracker_Colab_2_0.ipynb`** â€” GPU-based inference  
- **`TRACKER GPU.ipynb`** â€” interactive version for testing and visualization  

Steps:
1. Mount your Google Drive:
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```
2. Clone the repository or upload the tracker folder.
3. Extract model weights (`ModelWeights.rar`):
   ```python
   from pyunpack import Archive
   Archive('/content/drive/MyDrive/TrackerTools/ModelWeights.rar').extractall('/content/tracker/')
   ```
4. Run the tracker:
   ```bash
   !python /content/tracker/TrackerYolov3-Colab.py -i /content/tracker/stitched.mp4 -o /content/tracker/
   ```

Output trajectories and logs are saved under:
```
/videos  â†’ processed videos with detections  
/logs    â†’ text and CSV files with position and detection metadata
```

---

## ğŸ§  Synchronization

After tracking, the trajectories can be synchronized with electrophysiology recordings using the **`SYNCHRONIZATION/`** and **`Stitch_Sync_Track/`** modules.  
The synchronization aligns behavioral timestamps with LFP or spike data exported from Trodes.

---

## ğŸ§¾ Notes

- For best GPU performance, ensure ONNX Runtime GPU and CUDA/cuDNN versions are correctly matched.  
- Tested configurations:
  - **ONNX Runtime GPU**: 1.8.1  
  - **CUDA**: 11.6  
  - **cuDNN**: 8.5.0.96  
- Supported input: stitched videos in `.mp4` format (typically 12-camera composites).  

---

## ğŸ“‚ Folder Structure
```
TRACKER/
â”‚
â”œâ”€â”€ TRACKER_GPU.py                 # GPU-based tracking script (local execution)
â”œâ”€â”€ TRACKER GPU.ipynb              # Interactive tracking notebook
â”œâ”€â”€ Tracker_Colab_2_0.ipynb        # Colab version for large-scale tracking
â”œâ”€â”€ ModelWeights.rar               # YOLOv3 model weights
â”œâ”€â”€ logs/                          # Tracking logs and outputs
â””â”€â”€ README.md                      # This file
```

---

## ğŸ§© Integration

This module is part of the full **HM_RAT** pipeline:
- [`video_stitching/`](../video_stitching) â†’ merges multi-camera recordings  
- [`SYNCHRONIZATION/`](../SYNCHRONIZATION) â†’ aligns tracking with electrophysiology  
- [`LFP_event_detection/`](../LFP_event_detection) â†’ detects ripples, spindles, delta events

---

## ğŸ“œ License  
Â© **Genzel Lab** â€” for research use only.  


The versions used by Giulia were onnruntime-gpu-1.8.1 and the latest versions of CUDA and cuDNN.
In the most recent issue about onnx predictions they were using CUDA 11.6 and cnDNN 8.5.0.96
